# UniReps Workshop

[Unifying Representations in Neural Models](https://unireps.org).

NeurIPS, New Orleans (USA)

December 15th 2023

## Introduction

We've gathered here a comprehensive set of resources on the mechanisms,
and extent of similarity in internal representationsrepresentation, for deep learning and neuroscience. Check it out and contribute with a pull request!

Join us on Slack!

## Educational Resources

### Computational Neuroscience

* [Dayan & Abbott’s Theoretical Neuroscience textbook](http://www.gatsby.ucl.ac.uk/~dayan/book/)
* [Gerstner’s Neuronal Dynamics textbook](https://neuronaldynamics.epfl.ch)
* [Principles of Computational Modelling in Neuroscience](https://www.cambridge.org/us/universitypress/subjects/life-sciences/neuroscience/principles-computational-modelling-neuroscience#I1SF2c5dOfZ12Fr0.97)

## Conferences and Workshops

* [UniReps Workshop Unifying Representations in Neural Models (2023)](https://unireps.org)
* [NeurIPS Workshop on Symmetry and Geometry in Neural Representations (2022-2023)](https://www.neurreps.org/about)
* [ICLR Workshop on Geometrical and Topological Representation Learning (2022)](https://gt-rl.github.io)
* [Workshop on Symmetry, Invariance and Neural Representations @ The Bernstein Conference (2022)](https://bernstein-network.de/bernstein-conference/program/satellite-workshops/symmetry-invariance-and-neural-representations/)
* [SVRHM Shared Visual Representations in Human & Machine Intelligence (2019-2022)](https://www.svrhm.com).

## Software Libraries

## Datasets

Open-Source Neuroscience Datasets

* [Dandi](https://www.dandiarchive.org)
* [International Brain Laboratory](https://www.internationalbrainlab.com/data)
* [Kavli Institute for Systems Neuroscience Grid Cell Database](https://www.ntnu.edu/kavli/research/grid-cell-data)
* [NeuroData Without Borders](https://www.nwb.org/example-datasets/)
* [OpenNeuro](https://openneuro.org)
* [CRCNS](https://crcns.org/data-sets)

## Papers

We have gathered here a collection of relevant papers. Please note that this is currently work-in-progress, we will try to add the correct venues and links as soon as possible. If you have a paper to add or want to contribute in filling the gaps, please submit a pull request.

### Measures

| Title | Venue |
|-|-|
| [Similarity of Neural Network Representations Revisited](https://arxiv.org/abs/1905.00414) | ICML 2019 |
| Generalized Shape Metrics on Neural Representations | N/A |
| Grounding Representation Similarity with Statistical Testing | arXiv [cs.LG] |
| Measuring similarity for clarifying layer difference in multiplex ad hoc duplex information networks | J. Informetr. |
 Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy | Mach. Learn. |
| Transferred Discrepancy: Quantifying the Difference Between Representations | arXiv [cs.LG] |
| Diversity creation methods: a survey and categorisation | Inf. Fusion |
| An analysis of diversity measures | Mach. Learn. |
| Understanding the Dynamics of DNNs Using Graph Modularity | arXiv [cs.CV] |
| Similarity of Neural Networks with Gradients | arXiv [cs.LG] |
| Representation Topology Divergence: A Method for Comparing Neural Network Representations | N/A |
| Inter-layer Information Similarity Assessment of Deep Neural Networks Via Topological Similarity and Persistence Analysis of Data Neighbour Dynamics | arXiv [cs.LG] |
| Understanding image representations by measuring their equivariance and equivalence | N/A |

### Zero-shot alignment

| Title | Venue |
|--------------------------------------------------|-|
| [Relative representations enable zero-shot latent space communication](https://arxiv.org/abs/2209.15430) | ICLR 2023 |
| [ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training](https://arxiv.org/abs/2210.01738) | NeurIPS 2023 |

### Contrastive learning

| Title | Venue |
|--------------------------------------------------|-|
| Connecting Multi-modal Contrastive Representations | NeurIPS 2023 |
| Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning | NeurIPS 2022 |
| [ULIP: Learning Unified Representation of Language, Image and Point Cloud for 3D Understanding](https://arxiv.org/abs/2212.05171) | CVPR 2023 |
| [Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model](https://arxiv.org/abs/2310.17653) | N/A |

### Linear mode connectivity and model merging

| Title | Venue |
|-|-|
| [Git Re-Basin: Merging Models modulo Permutation Symmetries](https://arxiv.org/abs/2209.04836) | ICLR 2023 |
| [Model Fusion via Optimal Transport](https://arxiv.org/abs/1910.05653) | NeurIPS 2020 |
| Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity | arXiv [cs.LG] |
| REPAIR: REnormalizing Permuted Activations for Interpolation Repair | arXiv [cs.LG] |
| Linear Mode Connectivity in Multitask and Continual Learning | N/A |
| Optimizing Mode Connectivity via Neuron Alignment | N/A |
| Traversing Between Modes in Function Space for Fast Ensembling | arXiv [cs.LG] |
| An Empirical Study of Multimodal Model Merging | arXiv [cs.CV] |
| Linear Mode Connectivity and the Lottery Ticket Hypothesis | N/A |

### Other -- to be sorted

| Title | Venue |
|--------------------------------------------------|-|
| [Domain Translation via Latent Space Mapping](https://arxiv.org/abs/2212.03361) |IJCNN 2023 |
| Do Vision Transformers See Like Convolutional Neural Networks? | N/A |
| Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth | N/A |
| LiT: Zero-Shot Transfer with Locked-image text Tuning | arXiv [cs.CV] |
| Bootstrapping Parallel Anchors for Relative Representations | arXiv [cs.LG] |
| Revisiting Model Stitching to Compare Neural Representations | N/A |
| Similarity and Matching of Neural Network Representations | N/A |
| Topology of Deep Neural Networks | J. Mach. Learn. Res. |
| On Linear Identifiability of Learned Representations | N/A |
| Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation | N/A |
| Identifiability Results for Multimodal Contrastive Learning | arXiv [cs.LG] |
| Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time | N/A |
| The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks | N/A |
| Editing Models with Task Arithmetic | arXiv [cs.LG] |
| Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models | arXiv [cs.LG] |
| RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks | arXiv [cs.LG] |
| Invariant Risk Minimization | arXiv [stat.ML] |
| Neural networks learn to magnify areas near decision boundaries | arXiv [cs.LG] |
| On a Novel Application of Wasserstein-Procrustes for Unsupervised Cross-Lingual Learning | arXiv [cs.CL] |
| Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling | N/A |
| Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs | N/A |
| Essentially No Barriers in Neural Network Energy Landscape | N/A |
| Topology and Geometry of Half-Rectified Network Optimization | N/A |
| Qualitatively characterizing neural network optimization problems | N/A |
| On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima | N/A |
| Reliability of CKA as a Similarity Measure in Deep Learning | arXiv [cs.LG] |
| Insights on representational similarity in neural networks with canonical correlation | N/A |
| Distributed and overlapping representations of faces and objects in ventral temporal cortex | Science |
| Content and cluster analysis: Assessing representational similarity in neural systems | Philos. Psychol. |
| What makes different people's representations alike: neural similarity space solves the problem of across-subject fMRI decoding | J. Cogn. Neurosci. |
| Contrastive Multiview Coding | arXiv [cs.CV] |
| Similarity of Neural Network Models: A Survey of Functional and Representational Measures | arXiv [cs. LG] |
| High-dimensional dynamics of generalization error in neural networks | arXiv [stat.ML] |
| Exact solutions to the nonlinear dynamics of learning in deep linear neural networks | N/A |
| Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN) | N/A |
| Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning | N/A |
| Variational Autoencoders and Nonlinear ICA: A Unifying Framework | N/A |
| Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks | N/A |
| Modular Networks: Learning to Decompose Neural Computation | N/A |
| Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization | arXiv [cs.LG] |
| High-resolution image reconstruction with latent diffusion models from human brain activity | N/A |
| Alignment with human representations supports robust few-shot learning | arXiv [cs.LG] |
| SHARCS: Shared Concept Space for Explainable Multimodal Learning | arXiv [cs. LG] |
| Prevalence of Neural Collapse during the terminal phase of deep learning training | arXiv [cs.LG] |
| Convergent Learning: Do different neural networks learn the same representations? | N/A |
| Controlling Text-to-Image Diffusion by Orthogonal Finetuning | arXiv [cs.CV] |
| CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution | arXiv [cs.CV] |
| Manifold alignment using Procrustes analysis | N/A |
| SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability | N/A |
| Grounding High Dimensional Representation Similarity by Comparing Decodability and Network Performance | N/A |
| On the Symmetries of Deep Learning Models and their Internal Representations | arXiv [cs.LG] |
| Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change | N/A |
| The Effects of Randomness on the Stability of Node Embeddings | arXiv [cs.LG] |
| Representational dissimilarity metric spaces for stochastic neural networks | arXiv [cs.LG] |
| Representational similarity analysis - connecting the branches of systems neuroscience | Front. Syst. Neurosci. |
| Representation of object similarity in human vision: psychophysics and a computational model | Vision Res. |
| Graph-Based Similarity of Neural Network Representations | arXiv [cs.LG] |
| Using distance on the Riemannian manifold to compare representations in brain and in models | Neuroimage |
| Mechanistic Mode Connectivity | arXiv [cs.LG] |
| Adaptive Geo-Topological Independence Criterion | arXiv [stat.ML] |
| Measuring and testing dependence by correlation of distances | arXiv [math.ST] |
| Feature learning in deep classifiers through Intermediate Neural Collapse | N/A |
| Bootstrapping Vision-Language Learning with Decoupled Language Pre-training | arXiv [cs.CV] |
| Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning | N/A |
| UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning | N/A |
| Understanding Weight Similarity of Neural Networks via Chain Normalization Rule and Hypothesis-Training-Testing | arXiv [cs.LG] |
| Understanding the Behaviour of Contrastive Loss | N/A |
| Clustering units in neural networks: upstream vs downstream information | arXiv [cs.LG] |
| Understanding metric-related pitfalls in image analysis validation | arXiv [cs.CV] |
| Launch and Iterate: Reducing Prediction Churn | N/A |
| Model Stability with Continuous Data Updates | arXiv [cs.CL] |
| Anti-Distillation: Improving reproducibility of deep networks | arXiv [cs.LG] |
| On the Reproducibility of Neural Network Predictions | arXiv [cs.LG] |
| Deep Ensembles: A Loss Landscape Perspective | arXiv [stat.ML] |
| Measuring the Instability of Fine-Tuning | arXiv [cs. CL] |
| Predictive Multiplicity in Classification | N/A |
| Rashomon Capacity: A Metric for Predictive Multiplicity in Classification | arXiv [cs.LG] |
| mCLIP: Multilingual CLIP via Cross-lingual Transfer | N/A |
| Learning to Decompose Visual Features with Latent Textual Prompts | arXiv [cs.CV] |
| Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages | arXiv [cs.CL] |
| Leveraging Task Structures for Improved Identifiability in Neural Network Representations | ArXiv preprint |
| Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation | arXiv [cs.LG] |
| Substance or Style: What Does Your Image Embedding Know? | arXiv [cs.LG] |
| On Privileged and Convergent Bases in Neural Network Representations | arXiv [cs.LG] |
| Text-To-Concept (and Back) via Cross-Model Alignment | arXiv [cs.CV] |
| Stitchable Neural Networks | arXiv [cs.LG] |
| A Multi-View Embedding Space for Modeling Internet Images, Tags, and their Semantics | arXiv [cs.CV] |
| Policy Stitching: Learning Transferable Robot Policies | arXiv [cs.RO] |
| GeRA: Label-Efficient Geometrically Regularized Alignment | N/A |
| Deep Incubation: Training Large Models by Divide-and-Conquering | arXiv [cs.CV] |
| Flamingo: a Visual Language Model for Few-Shot Learning | arXiv [cs.CV] |
