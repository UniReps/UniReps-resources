# UniReps Workshop

[Unifying Representations in Neural Models](https://unireps.org).

NeurIPS, New Orleans (USA)

December 15th 2023

## Introduction

We've gathered here a comprehensive set of resources on the mechanisms,
and extent of similarity in internal representations, for deep learning and neuroscience. Check it out and contribute with a pull request!

Join us on Slack!

## Educational Resources

### Computational Neuroscience

* [Dayan & Abbott’s Theoretical Neuroscience textbook](http://www.gatsby.ucl.ac.uk/~dayan/book/)
* [Gerstner’s Neuronal Dynamics textbook](https://neuronaldynamics.epfl.ch)
* [Principles of Computational Modelling in Neuroscience](https://www.cambridge.org/us/universitypress/subjects/life-sciences/neuroscience/principles-computational-modelling-neuroscience#I1SF2c5dOfZ12Fr0.97)

## Conferences and Workshops

* [UniReps Workshop Unifying Representations in Neural Models (2023)](https://unireps.org)
* [NeurIPS Workshop on Symmetry and Geometry in Neural Representations (2022-2023)](https://www.neurreps.org/about)
* [ICLR Workshop on Geometrical and Topological Representation Learning (2022)](https://gt-rl.github.io)
* [Workshop on Symmetry, Invariance and Neural Representations @ The Bernstein Conference (2022)](https://bernstein-network.de/bernstein-conference/program/satellite-workshops/symmetry-invariance-and-neural-representations/)
* [SVRHM Shared Visual Representations in Human & Machine Intelligence (2019-2022)](https://www.svrhm.com).

## Software Libraries

## Datasets

Open-Source Neuroscience Datasets

* [Dandi](https://www.dandiarchive.org)
* [International Brain Laboratory](https://www.internationalbrainlab.com/data)
* [Kavli Institute for Systems Neuroscience Grid Cell Database](https://www.ntnu.edu/kavli/research/grid-cell-data)
* [NeuroData Without Borders](https://www.nwb.org/example-datasets/)
* [OpenNeuro](https://openneuro.org)
* [CRCNS](https://crcns.org/data-sets)

## Papers

We have gathered here a collection of relevant papers. Please note that this is currently work-in-progress, we will try to add the correct venues and links as soon as possible. If you have a paper to add or want to contribute in filling the gaps, please submit a pull request.

### Measures

| Title | Venue |
|-|-|
| [Similarity of Neural Network Representations Revisited](https://arxiv.org/abs/1905.00414) | ICML 2019 |
| [Generalized Shape Metrics on Neural Representations](https://arxiv.org/abs/2110.14739) | NeurIPS 2021 |
| [Grounding Representation Similarity with Statistical Testing](https://arxiv.org/abs/2108.01661) | NeurIPS 2021 |
| [SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability](https://arxiv.org/abs/1706.05806) | NeurIPS 2017 |
| [Measuring similarity for clarifying layer difference in multiplex ad hoc duplex information networks](https://www.sciencedirect.com/science/article/abs/pii/S1751157719301890) | J. Informetr. |
| [Reliability of CKA as a Similarity Measure in Deep Learning](https://arxiv.org/abs/2210.16156) | ICLR 2023 |
| [Transferred Discrepancy: Quantifying the Difference Between Representations](https://arxiv.org/abs/2007.12446) | arXiv [cs.LG] |
| [Diversity creation methods: a survey and categorisation](https://www.sciencedirect.com/science/article/abs/pii/S1566253504000375) | Inf. Fusion |
| [An analysis of diversity measures](https://link.springer.com/article/10.1007/s10994-006-9449-2) | Mach. Learn. |
| [Understanding the Dynamics of DNNs Using Graph Modularity](https://link.springer.com/chapter/10.1007/978-3-031-19775-8_14) | arXiv [cs.CV] |
| [Similarity of Neural Networks with Gradients](https://arxiv.org/abs/2003.11498) | arXiv [cs.LG] |
| [Representation Topology Divergence: A Method for Comparing Neural Network Representations](https://arxiv.org/abs/2201.00058) | arXiv [cs.LG]  |
| [Inter-layer Information Similarity Assessment of Deep Neural Networks Via Topological Similarity and Persistence Analysis of Data Neighbour Dynamics](https://arxiv.org/abs/2012.03793) | arXiv [cs.LG]|
| [Understanding image representations by measuring their equivariance and equivalence](https://arxiv.org/abs/1411.5908) | arXiv [cs.LG]  |
| [Insights on representational similarity in neural networks with canonical correlation](https://arxiv.org/abs/1806.05759) | arXiv |
| [Understanding metric-related pitfalls in image analysis validation](https://arxiv.org/abs/2302.01790) | arXiv [cs.CV] |
| [Revisiting Model Stitching to Compare Neural Representations](https://arxiv.org/abs/2106.07682) | arXiv  |
| [Similarity and Matching of Neural Network Representations](https://arxiv.org/abs/2110.14633) | arXiv |
| [Topology of Deep Neural Networks](https://arxiv.org/abs/2110.14633) | J. Mach. Learn. Res. |
| [Representational dissimilarity metric spaces for stochastic neural networks](https://arxiv.org/abs/2211.11665) | arXiv [cs.LG] |
| [Graph-Based Similarity of Neural Network Representations](https://arxiv.org/abs/2111.11165) | arXiv [cs.LG] |
| [Adaptive Geo-Topological Independence Criterion](https://arxiv.org/abs/1810.02923) | arXiv [stat.ML] |
|[Using distance on the Riemannian manifold to compare representations in brain and in models Neuroimage](https://www.sciencedirect.com/science/article/pii/S1053811921005474) |
| [Predictive Multiplicity in Classification](https://arxiv.org/abs/1909.06677) | arXiv [cs.ML] |
| [Rashomon Capacity: A Metric for Predictive Multiplicity in Classification](https://arxiv.org/abs/2206.01295) | arXiv [cs.LG] |
| [Understanding Weight Similarity of Neural Networks via Chain Normalization Rule and Hypothesis-Training-Testing](https://arxiv.org/abs/2208.04369) | arXiv [cs.LG] |
| [Grounding High Dimensional Representation Similarity by Comparing Decodability and Network Performance](https://openreview.net/forum?id=QHiuyzE69Bx) | OpenReview |
| [Measures of Diversity in Classifier Ensembles and Their Relationship with the Ensemble Accuracy](https://dl.acm.org/doi/10.1023/A%3A1022859003006) | Mach. Learn. |

### Alignment and zero-shot alignment

| Title | Venue |
|--------------------------------------------------|-|
| [Relative representations enable zero-shot latent space communication](https://arxiv.org/abs/2209.15430) | ICLR 2023 |
| [ASIF: Coupled Data Turns Unimodal Models to Multimodal Without Training](https://arxiv.org/abs/2210.01738) | NeurIPS 2023 |
| [Manifold alignment using Procrustes analysis](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1061&context=cs_faculty_pubs) | arXiv |
| [Bootstrapping Parallel Anchors for Relative Representations](https://arxiv.org/abs/2303.00721) | arXiv [cs.LG] |
| [Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages](https://arxiv.org/abs/2306.16774) | arXiv [cs.CL] |
| [GeRA: Label-Efficient Geometrically Regularized Alignment](https://arxiv.org/abs/2310.00672#:~:text=Pretrained%20unimodal%20encoders%20incorporate%20rich,data%20for%20alignment%20and%20training.) | arXiv |
| [Text-To-Concept (and Back) via Cross-Model Alignment](https://arxiv.org/abs/2305.06386) | arXiv [cs.CV] |

### Contrastive learning

| Title | Venue |
|--------------------------------------------------|-|
| [Connecting Multi-modal Contrastive Representations](https://arxiv.org/abs/2305.14381) | NeurIPS 2023 |
| [Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning](https://arxiv.org/abs/2203.02053) | NeurIPS 2022 |
| [ULIP: Learning Unified Representation of Language, Image and Point Cloud for 3D Understanding](https://arxiv.org/abs/2212.05171) | CVPR 2023 |
| [Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model](https://arxiv.org/abs/2310.17653) | N/A |
| [Identifiability Results for Multimodal Contrastive Learning](https://arxiv.org/abs/2303.09166) | ICLR 2023 |
| [UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning](https://arxiv.org/abs/2012.15409) | N/A |
| [Understanding the Behaviour of Contrastive Loss](https://arxiv.org/abs/2012.09740) | N/A |

### Linear mode connectivity and model merging

| Title | Venue |
|-|-|
| [Git Re-Basin: Merging Models modulo Permutation Symmetries](https://arxiv.org/abs/2209.04836) | ICLR 2023 |
| [Model Fusion via Optimal Transport](https://arxiv.org/abs/1910.05653) | NeurIPS 2020 |
| [Going Beyond Linear Mode Connectivity: The Layerwise Linear Feature Connectivity](https://arxiv.org/abs/2307.08286) | arXiv [cs.LG] |
| [REPAIR: REnormalizing Permuted Activations for Interpolation Repair](https://arxiv.org/abs/2211.08403) | arXiv [cs.LG] |
| [Linear Mode Connectivity in Multitask and Continual Learning](https://arxiv.org/abs/2010.04495) | arXiv |
| [Optimizing Mode Connectivity via Neuron Alignment](https://proceedings.neurips.cc/paper/2020/file/aecad42329922dfc97eee948606e1f8e-Paper.pdf) | arXiv |
| [Traversing Between Modes in Function Space for Fast Ensembling](https://arxiv.org/abs/2306.11304) | arXiv [cs.LG] |
| [An Empirical Study of Multimodal Model Merging](https://arxiv.org/abs/2304.14933) | arXiv [cs.CV] |
| [Linear Mode Connectivity and the Lottery Ticket Hypothesis](https://arxiv.org/abs/1912.05671) | arXiv |
| [Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling](https://proceedings.mlr.press/v139/benton21a/benton21a.pdf) | arXiv |
| [Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs](Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs) | arXiv |
| [The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks](https://arxiv.org/abs/2110.06296) | arXiv |
| [Essentially No Barriers in Neural Network Energy Landscape](https://arxiv.org/abs/1803.00885) | arXiv |

### Neuroscience

| Title | Venue |
|-|-|
| [Representational similarity analysis - connecting the branches of systems neuroscience](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/) | Front. Syst. Neurosci. |
| [What makes different people's representations alike: neural similarity space solves the problem of across-subject fMRI decoding](https://pubmed.ncbi.nlm.nih.gov/22220728/) | J. Cogn. Neurosci. |
| [Distributed and overlapping representations of faces and objects in ventral temporal cortex](https://pubmed.ncbi.nlm.nih.gov/11577229/) | Science |

### Other -- to be sorted

| Title | Venue |
|--------------------------------------------------|-|
| [Domain Translation via Latent Space Mapping](https://arxiv.org/abs/2212.03361) |IJCNN 2023 |
| [Do Vision Transformers See Like Convolutional Neural Networks?](https://arxiv.org/abs/2108.08810) | N/A |
| [Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth](https://arxiv.org/abs/2010.15327) | N/A |
| [LiT: Zero-Shot Transfer with Locked-image text Tuning](https://arxiv.org/abs/2111.07991) | arXiv [cs.CV] |
| [On Linear Identifiability of Learned Representations](https://arxiv.org/abs/2007.00810) | N/A |
| [Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation](https://www.researchgate.net/publication/328598982_Towards_Understanding_Learning_Representations_To_What_Extent_Do_Different_Neural_Networks_Learn_the_Same_Representation) | N/A |
| [Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time](https://arxiv.org/abs/2203.05482) | ICML 2022 |
| [Editing Models with Task Arithmetic](https://arxiv.org/abs/2212.04089) | ICLR 2023 |
| [Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models](https://arxiv.org/abs/2305.12827) | arXiv [cs.LG] |
| [RNNs of RNNs: Recursive Construction of Stable Assemblies of Recurrent Neural Networks](https://arxiv.org/abs/2106.08928) | arXiv [cs.LG] |
| [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893) | arXiv [stat.ML] |
| [Neural networks learn to magnify areas near decision boundaries](https://arxiv.org/abs/2301.11375) | arXiv [cs.LG] |
| [On a Novel Application of Wasserstein-Procrustes for Unsupervised Cross-Lingual Learning](https://arxiv.org/abs/2007.09456#:~:text=18%20Jul%202020%5D-,On%20a%20Novel%20Application%20of%20Wasserstein,for%20Unsupervised%20Cross%2DLingual%20Learning&text=The%20emergence%20of%20unsupervised%20word,Natural%20Language%20Processing%20(NLP).) | arXiv [cs.CL] |
| [Topology and Geometry of Half-Rectified Network Optimization](https://arxiv.org/abs/1611.01540) | N/A |
| [Qualitatively characterizing neural network optimization problems](https://arxiv.org/abs/1412.6544) | N/A |
| [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836) | N/A |
| [Content and cluster analysis: Assessing representational similarity in neural systems](https://www.tandfonline.com/doi/abs/10.1080/09515080050002726) | Philos. Psychol. |
| [Contrastive Multiview Coding](https://arxiv.org/abs/1906.05849) | arXiv [cs.CV] |
| Similarity of Neural Network Models: A Survey of Functional and Representational Measures | arXiv [cs. LG] |
| High-dimensional dynamics of generalization error in neural networks | arXiv [stat.ML] |
| Exact solutions to the nonlinear dynamics of learning in deep linear neural networks | N/A |
| Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN) | N/A |
| Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning | N/A |
| Variational Autoencoders and Nonlinear ICA: A Unifying Framework | N/A |
| Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks | N/A |
| Modular Networks: Learning to Decompose Neural Computation | N/A |
| Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization | arXiv [cs.LG] |
| High-resolution image reconstruction with latent diffusion models from human brain activity | N/A |
| Alignment with human representations supports robust few-shot learning | arXiv [cs.LG] |
| SHARCS: Shared Concept Space for Explainable Multimodal Learning | arXiv [cs. LG] |
| Prevalence of Neural Collapse during the terminal phase of deep learning training | arXiv [cs.LG] |
| Convergent Learning: Do different neural networks learn the same representations? | N/A |
| Controlling Text-to-Image Diffusion by Orthogonal Finetuning | arXiv [cs.CV] |
| CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution | arXiv [cs.CV] |
| On the Symmetries of Deep Learning Models and their Internal Representations | arXiv [cs.LG] |
| Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change | N/A |
| The Effects of Randomness on the Stability of Node Embeddings | arXiv [cs.LG] |
| Representation of object similarity in human vision: psychophysics and a computational model | Vision Res. |
| Mechanistic Mode Connectivity | arXiv [cs.LG] |
| Feature learning in deep classifiers through Intermediate Neural Collapse | N/A |
| Bootstrapping Vision-Language Learning with Decoupled Language Pre-training | arXiv [cs.CV] |
| Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning | N/A |
| Clustering units in neural networks: upstream vs downstream information | arXiv [cs.LG] |
| Launch and Iterate: Reducing Prediction Churn | N/A |
| Model Stability with Continuous Data Updates | arXiv [cs.CL] |
| Anti-Distillation: Improving reproducibility of deep networks | arXiv [cs.LG] |
| On the Reproducibility of Neural Network Predictions | arXiv [cs.LG] |
| Deep Ensembles: A Loss Landscape Perspective | arXiv [stat.ML] |
| Measuring the Instability of Fine-Tuning | arXiv [cs. CL] |
| mCLIP: Multilingual CLIP via Cross-lingual Transfer | N/A |
| Learning to Decompose Visual Features with Latent Textual Prompts | arXiv [cs.CV] |
| Leveraging Task Structures for Improved Identifiability in Neural Network Representations | ArXiv preprint |
| Additive Decoders for Latent Variables Identification and Cartesian-Product Extrapolation | arXiv [cs.LG] |
| Substance or Style: What Does Your Image Embedding Know? | arXiv [cs.LG] |
| On Privileged and Convergent Bases in Neural Network Representations | arXiv [cs.LG] |
| Stitchable Neural Networks | arXiv [cs.LG] |
| A Multi-View Embedding Space for Modeling Internet Images, Tags, and their Semantics | arXiv [cs.CV] |
| Policy Stitching: Learning Transferable Robot Policies | arXiv [cs.RO] |
| Deep Incubation: Training Large Models by Divide-and-Conquering | arXiv [cs.CV] |
| Flamingo: a Visual Language Model for Few-Shot Learning | arXiv [cs.CV] |
